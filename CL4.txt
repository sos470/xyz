PB1:
1)Open Power BI Desktop.

2)Click Home → Get Data.

3)Choose your source:
	
	For Excel: Select Excel → Browse and import file.


4)Select desired tables/sheets → Click Load to bring data into Power BI.



PB2: 
1)Click Transform Data to open Power Query Editor.

2)Perform operations like:
FOR PRODUCTS
	1.Remove columns	Right-click column → Remove others ( keep: Product id , product name , Quantity per unit , units in stock)

	2.Remove Nulls	Home → Remove Rows → Remove Blank Rows	Rows with missing values

	3.Create New Column	Add Column → Custom Column	Total = Products[UnitsInStock] * Products[UnitPrice]

FOR ORDERS
	1.Select Recent sources > Paste the od link > Go to Order Details Coloumns > Go to expand > Keep this columns (Product id , unit price , Quantity)    
	
	2.Create New Column	Add Column → Custom Column 
	  Use formula (TotalSales = Orders[Order_Details.Quantity] * 	Orders[Order_Details.UnitPrice] )

	3.Change Data Type > Column header icon > Date, Number , Text

	4.Modelling > Manage Relationships > Click box > Close

	5.Apply Changes	Home → Close & Apply	Finalize ETL and return to report view

	


PB3:
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd

iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print("\nSample predictions:\n", df.head())



PB4:
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

iris = load_iris()
X = iris.data
df = pd.DataFrame(X, columns=iris.feature_names)

kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)
df['Cluster'] = kmeans.labels_

sns.scatterplot(x=df[iris.feature_names[0]],
                y=df[iris.feature_names[1]],
                hue=df['Cluster'],
                palette='Set1')
plt.title('K-Means Clustering of Iris Dataset')
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1])
plt.legend(title='Cluster')
plt.show()



PB5:
text = """
MapReduce is a programming model for processing large data sets with a parallel, distributed algorithm on a cluster.
MapReduce consists of a Map procedure and a Reduce procedure.
"""

def map_function(text):
    mapped = []
    for line in text.strip().split('\n'):
        words = line.lower().split()
        for word in words:
            cleaned_word = ''.join(filter(str.isalnum, word))
            if cleaned_word:
                mapped.append((cleaned_word, 1))
    return mapped

def shuffle_and_sort(mapped):
    grouped = {}
    for word, count in mapped:
        if word in grouped:
            grouped[word].append(count)
        else:
            grouped[word] = [count]
    return grouped

def reduce_function(grouped):
    reduced = {}
    for word in grouped:
        reduced[word] = sum(grouped[word])
    return reduced

mapped = map_function(text)
grouped = shuffle_and_sort(mapped)
reduced = reduce_function(grouped)

for word in sorted(reduced):
    print(f"{word}: {reduced[word]}")

def map_reduce_all_words(file_path):
    with open(file_path, 'r') as file:
        mapped_data = [(word.lower(), 1) for line in file for word in line.strip().split()]
        
    reduced_data = {}
    for word, count in mapped_data:
        if word in reduced_data:
            reduced_data[word] += count
        else:
            reduced_data[word] = count
    return reduced_data

file_path = 'word-freq.txt'
word_frequencies = map_reduce_all_words(file_path)

print("\nAll word frequencies:")
for word in sorted(word_frequencies):
    print(f"{word}: {word_frequencies[word]}")

def map_reduce_target_word(file_path, target_word):
    word_count = 0
    with open(file_path, 'r') as file:
        mapped_data = [(word.lower(), 1) for line in file for word in line.strip().split()]
        
    for word, count in mapped_data:
        if word == target_word.lower():
            word_count += count
            
    return word_count

file_path = 'word-freq.txt'
target_word = "Girish"
frequency = map_reduce_target_word(file_path, target_word)
print(f"The Word '{target_word}' appears {frequency} times.")


PB6:
1) Load excel (Superstore) > Select all 3 coloumns 
2) Choose Options From visualization: Slicer , Pie Chart , KPI , Bar Chart, Etc.
3) Show Model View


PB7:
def map_phase(file_path):
    mapped_data = []
    with open(file_path, 'r') as file:
        for line in file:
            parts = line.strip().split()
            if len(parts) == 2:
                name, marks = parts
                try:
                    marks = int(marks)
                    mapped_data.append((name, marks))
                except ValueError:
                    pass
    return mapped_data

def reduce_phase(mapped_data):
    def get_grade(marks):
        if marks >= 90:
            return 'A'
        elif marks >= 80:
            return 'B'
        elif marks >= 70:
            return 'C'
        elif marks >= 60:
            return 'D'
        else:
            return 'F'
        
    reduced_data = {}
    for name, marks in mapped_data:
        reduced_data[name] = get_grade(marks)
    return reduced_data


file_path = 'student_scores_multi_names.txt'

mapped = map_phase(file_path)
grades = reduce_phase(mapped)

print("Students Grades:")
for name in sorted(grades):
    print(f"{name}: {grades[name]}")

PB8: 
import pandas as pd

def map_phase(file_path):
    df = pd.read_csv(file_path)
    
    males_died = df[(['Survived'] == 0) & (df['Sex'] == 'male')]
    
    females_died = df[(df['Survived'] == 0) & (df['Sex'] == 'female')]
    
    males_ages = males_died['Age'].dropna()
    
    female_class_count = females_died['Pclass'].value_counts()
    
    return males_ages, female_class_count

def reduce_phase(male_ages, female_class_count):
    if not male_ages.empty:
        avg_age = male_ages.mean()
    else:
        avg_age = 0
        
    return round(avg_age, 2), female_class_count

file_path = 'titanic_dataset.csv'

males_ages, female_deaths_by_class = map_phase(file_path)
average_males_age, female_deaths_by_class = reduce_phase(males_ages, female_deaths_by_class)

print(f"\nAverage age of males who died: {average_males_age}")
print("\nNumber of females who died in each class:")
for cls, count in female_deaths_by_class.items():
    print(f"Class {cls}: {count}")


PB9:
https://www.mycompiler.io/new/mongodb

// 1. CREATE – insert a new document
db.users.insertOne({
  name: "Alice",
  email: "alice@example.com",
  age: 28,
  roles: ["editor", "subscriber"]
});

// you can also insert many at once:
db.users.insertMany([
  { name: "Bob",   email: "bob@example.com",   age: 35, roles: ["admin"] },
  { name: "Carol", email: "carol@example.com", age: 22, roles: ["subscriber"] }
]);

// 2. READ – query documents
// find all users
db.users.find();

// find users with age > 25
db.users.find({ age: { $gt: 25 } });

// find one by email
db.users.findOne({ email: "alice@example.com" });

// project only name and email fields
db.users.find({}, { name: 1, email: 1, _id: 0 });

// 3. UPDATE – modify existing documents
// update one document (set a new field, increment age)
db.users.updateOne(
  { name: "Alice" },                   // filter
  { 
    $set:   { status: "active" },      // add/overwrite fields
    $inc:   { age: 1 }                 // increment numeric fields
  }
);

// update many (add a role to every subscriber)
db.users.updateMany(
  { roles: "subscriber" },
  { $addToSet: { roles: "newsletter" } }
);

// replace a whole document
db.users.replaceOne(
  { email: "bob@example.com" },
  { name: "Robert", email: "bob@example.com", age: 36, roles: ["admin", "editor"] }
);

// 4. DELETE – remove documents
// delete one
db.users.deleteOne({ name: "Carol" });

// delete many (remove all subscribers)
db.users.deleteMany({ roles: "subscriber" });

// drop the entire collection
db.users.drop();